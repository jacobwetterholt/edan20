{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words Sequences\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import regex as re\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Selma Lagerlöf\\n\\n\\nNils Holgerssons underbara resa g'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = '/Users/jacobwetterholt/uni/4/sprak/edan20/labs_2023/Selma.txt'\n",
    "text = open(file_name).read().strip()\n",
    "text[:50]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    words = re.findall(r'\\p{L}+', text)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to count the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unigrams(words):\n",
    "    frequency = {}\n",
    "    for word in words:\n",
    "        if word in frequency:\n",
    "            frequency[word] += 1\n",
    "        else:\n",
    "            frequency[word] = 1\n",
    "    return frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze Selma Lagerlöf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "och \t 36356\n",
      "att \t 28020\n",
      "han \t 21589\n",
      "det \t 21108\n",
      "i \t 16508\n",
      "som \t 16288\n",
      "på \t 14250\n",
      "en \t 13514\n",
      "hon \t 13313\n",
      "hade \t 13198\n",
      "var \t 12090\n",
      "de \t 11942\n",
      "den \t 11624\n",
      "inte \t 11355\n",
      "jag \t 9511\n"
     ]
    }
   ],
   "source": [
    "words = tokenize(text.lower())\n",
    "frequency = count_unigrams(words)\n",
    "for word in sorted(frequency.keys(), key=frequency.get, reverse=True)[:15]:\n",
    "    print(word, '\\t', frequency[word])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend the counts to pairs of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bigrams(words):\n",
    "    bigrams = [tuple(words[idx:idx + 2])\n",
    "               for idx in range(len(words) - 1)]\n",
    "    frequencies = {}\n",
    "    for bigram in bigrams:\n",
    "        if bigram in frequencies:\n",
    "            frequencies[bigram] += 1\n",
    "        else:\n",
    "            frequencies[bigram] = 1\n",
    "    return frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('det', 'var') \t 3840\n",
      "('att', 'han') \t 2958\n",
      "('för', 'att') \t 2932\n",
      "('han', 'hade') \t 2045\n",
      "('att', 'det') \t 2038\n",
      "('det', 'är') \t 2015\n",
      "('att', 'hon') \t 1751\n",
      "('att', 'de') \t 1331\n",
      "('så', 'att') \t 1264\n",
      "('hon', 'hade') \t 1254\n",
      "('att', 'jag') \t 1151\n",
      "('han', 'var') \t 987\n",
      "('var', 'det') \t 962\n",
      "('på', 'att') \t 876\n",
      "('som', 'han') \t 867\n"
     ]
    }
   ],
   "source": [
    "words = tokenize(text.lower())\n",
    "frequency_bigrams = count_bigrams(words)\n",
    "for bigram in sorted(frequency_bigrams.keys(), key=frequency_bigrams.get, reverse=True)[:15]:\n",
    "    print(bigram, '\\t', frequency_bigrams[bigram])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trigrams(words):\n",
    "    trigrams = [tuple(words[idx:idx + 3])\n",
    "                for idx in range(len(words) - 2)]\n",
    "    frequencies = {}\n",
    "    for trigram in trigrams:\n",
    "        if trigram in frequencies:\n",
    "            frequencies[trigram] += 1\n",
    "        else:\n",
    "            frequencies[trigram] = 1\n",
    "    return frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('att', 'det', 'var') \t 535\n",
      "('att', 'han', 'hade') \t 364\n",
      "('att', 'han', 'inte') \t 334\n",
      "('det', 'var', 'en') \t 330\n",
      "('att', 'han', 'skulle') \t 322\n",
      "('i', 'alla', 'fall') \t 262\n",
      "('det', 'var', 'inte') \t 257\n",
      "('men', 'det', 'var') \t 256\n",
      "('och', 'det', 'var') \t 240\n",
      "('så', 'att', 'han') \t 224\n",
      "('att', 'hon', 'skulle') \t 217\n",
      "('för', 'att', 'få') \t 211\n",
      "('att', 'han', 'var') \t 205\n",
      "('som', 'han', 'hade') \t 205\n",
      "('det', 'var', 'som') \t 201\n",
      "('att', 'det', 'är') \t 201\n",
      "('att', 'hon', 'hade') \t 195\n",
      "('att', 'hon', 'inte') \t 190\n",
      "('att', 'det', 'inte') \t 188\n",
      "('som', 'om', 'han') \t 183\n",
      "('därför', 'att', 'han') \t 154\n",
      "('än', 'en', 'gång') \t 150\n",
      "('det', 'är', 'inte') \t 149\n",
      "('för', 'att', 'se') \t 145\n",
      "('på', 'samma', 'gång') \t 143\n",
      "('att', 'tänka', 'på') \t 139\n",
      "('så', 'att', 'det') \t 139\n",
      "('sig', 'för', 'att') \t 138\n",
      "('att', 'de', 'skulle') \t 138\n",
      "('att', 'de', 'hade') \t 133\n"
     ]
    }
   ],
   "source": [
    "words = tokenize(text.lower())\n",
    "frequency_trigrams = count_trigrams(words)\n",
    "for trigram in sorted(frequency_trigrams.keys(), key=frequency_trigrams.get, reverse=True)[:30]:\n",
    "    print(trigram, '\\t', frequency_trigrams[trigram])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries do not guarantee the order. We can sort according to the frequency and then the lexical order using a `lambda` function to define the sorting key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('att', 'det', 'var') \t 535\n",
      "('att', 'han', 'hade') \t 364\n",
      "('att', 'han', 'inte') \t 334\n",
      "('det', 'var', 'en') \t 330\n",
      "('att', 'han', 'skulle') \t 322\n",
      "('i', 'alla', 'fall') \t 262\n",
      "('det', 'var', 'inte') \t 257\n",
      "('men', 'det', 'var') \t 256\n",
      "('och', 'det', 'var') \t 240\n",
      "('så', 'att', 'han') \t 224\n",
      "('att', 'hon', 'skulle') \t 217\n",
      "('för', 'att', 'få') \t 211\n",
      "('att', 'han', 'var') \t 205\n",
      "('som', 'han', 'hade') \t 205\n",
      "('att', 'det', 'är') \t 201\n",
      "('det', 'var', 'som') \t 201\n",
      "('att', 'hon', 'hade') \t 195\n",
      "('att', 'hon', 'inte') \t 190\n",
      "('att', 'det', 'inte') \t 188\n",
      "('som', 'om', 'han') \t 183\n",
      "('därför', 'att', 'han') \t 154\n",
      "('än', 'en', 'gång') \t 150\n",
      "('det', 'är', 'inte') \t 149\n",
      "('för', 'att', 'se') \t 145\n",
      "('på', 'samma', 'gång') \t 143\n",
      "('att', 'tänka', 'på') \t 139\n",
      "('så', 'att', 'det') \t 139\n",
      "('att', 'de', 'skulle') \t 138\n",
      "('sig', 'för', 'att') \t 138\n",
      "('att', 'de', 'hade') \t 133\n"
     ]
    }
   ],
   "source": [
    "for trigram in sorted(frequency_trigrams.keys(), key=lambda x: (-frequency_trigrams.get(x), x))[:30]:\n",
    "    print(trigram, '\\t', frequency_trigrams[trigram])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams(words, n):\n",
    "    ngrams = [tuple(words[idx:idx + n])\n",
    "              for idx in range(len(words) - n + 1)]\n",
    "    # \"\\t\".join(words[idx:idx + n])\n",
    "    frequencies = {}\n",
    "    for ngram in ngrams:\n",
    "        if ngram in frequencies:\n",
    "            frequencies[ngram] += 1\n",
    "        else:\n",
    "            frequencies[ngram] = 1\n",
    "    return frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('haver', 'sagt', 'österrike', 'portugal', 'metz', 'japan', 'som', 'det', 'var', 'bom') \t 6\n",
      "('japan', 'som', 'det', 'var', 'bom', 'bom', 'bom', 'å', 'rulla', 'bom') \t 6\n",
      "('metz', 'japan', 'som', 'det', 'var', 'bom', 'bom', 'bom', 'å', 'rulla') \t 6\n",
      "('portugal', 'metz', 'japan', 'som', 'det', 'var', 'bom', 'bom', 'bom', 'å') \t 6\n",
      "('sagt', 'österrike', 'portugal', 'metz', 'japan', 'som', 'det', 'var', 'bom', 'bom') \t 6\n",
      "('som', 'det', 'var', 'bom', 'bom', 'bom', 'å', 'rulla', 'bom', 'bom') \t 6\n",
      "('som', 'tidningen', 'haver', 'sagt', 'österrike', 'portugal', 'metz', 'japan', 'som', 'det') \t 6\n",
      "('tidningen', 'haver', 'sagt', 'österrike', 'portugal', 'metz', 'japan', 'som', 'det', 'var') \t 6\n",
      "('österrike', 'portugal', 'metz', 'japan', 'som', 'det', 'var', 'bom', 'bom', 'bom') \t 6\n",
      "('han', 'satt', 'i', 'nästa', 'rum', 'och', 'så', 'kastade', 'han', 'sig') \t 5\n",
      "('i', 'nästa', 'rum', 'och', 'så', 'kastade', 'han', 'sig', 'över', 'oss') \t 5\n",
      "('satt', 'i', 'nästa', 'rum', 'och', 'så', 'kastade', 'han', 'sig', 'över') \t 5\n",
      "('att', 'säga', 'hur', 'de', 'togo', 'sig', 'ut', 'på', 'nära', 'håll') \t 4\n",
      "('bara', 'och', 'språkade', 'han', 'satt', 'i', 'nästa', 'rum', 'och', 'så') \t 4\n",
      "('folk', 'vill', 'lyda', 'mitt', 'råd', 'bör', 'det', 'genast', 'flytta', 'mot') \t 4\n"
     ]
    }
   ],
   "source": [
    "words = tokenize(text.lower())\n",
    "frequency_ngrams = count_ngrams(words, N)\n",
    "for ngram in sorted(frequency_ngrams.keys(), key=lambda x: (-frequency_ngrams.get(x), x))[:15]:\n",
    "    print(ngram, '\\t', frequency_ngrams[ngram])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cooccurrence measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the computations, we need this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = count_unigrams(words)\n",
    "frequency_bigrams = count_bigrams(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_info(words, freq_unigrams, freq_bigrams):\n",
    "    mi = {}\n",
    "    factor = len(words) * len(words) / (len(words) - 1)\n",
    "    for bigram in freq_bigrams:\n",
    "        mi[bigram] = (\n",
    "            math.log(factor * freq_bigrams[bigram] /\n",
    "                     (freq_unigrams[bigram[0]] *\n",
    "                      freq_unigrams[bigram[1]]), 2))\n",
    "    return mi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = mutual_info(words, frequency, frequency_bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual information is highly biased toward low-frequency words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 5\n",
    "filtered_mi = {k: v for k, v in mi.items() if frequency_bigrams[k] >= cutoff}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('el', 'aksa') \t 7 \t 6 \t 6 \t 17.009375642572188\n",
      "('metz', 'japan') \t 6 \t 7 \t 6 \t 17.009375642572188\n",
      "('théâtre', 'français') \t 7 \t 7 \t 7 \t 17.009375642572188\n",
      "('monsieur', 'bartout') \t 8 \t 5 \t 5 \t 16.81673056462979\n",
      "('portugal', 'metz') \t 8 \t 6 \t 6 \t 16.81673056462979\n",
      "('rättar', 'söderlind') \t 8 \t 5 \t 5 \t 16.81673056462979\n",
      "('österrike', 'portugal') \t 6 \t 8 \t 6 \t 16.81673056462979\n",
      "('omars', 'moské') \t 7 \t 8 \t 6 \t 16.594338143293342\n",
      "('neljä', 'viisi') \t 7 \t 7 \t 5 \t 16.523948815401944\n",
      "('bonniers', 'förlag') \t 11 \t 11 \t 11 \t 16.357298945992493\n",
      "('tidiga', 'morgonstunden') \t 11 \t 5 \t 5 \t 16.357298945992493\n",
      "('l', 'univers') \t 12 \t 9 \t 9 \t 16.231768063908635\n",
      "('britta', 'lambert') \t 13 \t 9 \t 9 \t 16.1162908464887\n",
      "('motorjakten', 'najaden') \t 6 \t 13 \t 6 \t 16.1162908464887\n",
      "('sophie', 'elkan') \t 12 \t 12 \t 11 \t 16.106237181824778\n"
     ]
    }
   ],
   "source": [
    "for bigram in sorted(filtered_mi.keys(), key=lambda x: (-filtered_mi.get(x), x))[:15]:\n",
    "    print(bigram, '\\t',\n",
    "          frequency[bigram[0]], '\\t',\n",
    "          frequency[bigram[1]], '\\t',\n",
    "          frequency_bigrams[bigram], '\\t',\n",
    "          filtered_mi[bigram])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_ratio(words, freq_unigrams, freq_bigrams):\n",
    "    lr = {}\n",
    "    for bigram in freq_bigrams:\n",
    "        p = freq_unigrams[bigram[1]] / len(words)\n",
    "        p1 = freq_bigrams[bigram] / freq_unigrams[bigram[0]]\n",
    "        p2 = ((freq_unigrams[bigram[1]] - freq_bigrams[bigram])\n",
    "              / (len(words) - freq_unigrams[bigram[0]]))\n",
    "        if p1 != 1.0 and p2 != 0.0:\n",
    "            lr[bigram] = 2.0 * (\n",
    "                log_f(freq_bigrams[bigram],\n",
    "                      freq_unigrams[bigram[0]], p1) +\n",
    "                log_f(freq_unigrams[bigram[1]] -\n",
    "                      freq_bigrams[bigram],\n",
    "                      len(words) - freq_unigrams[bigram[0]], p2) -\n",
    "                log_f(freq_bigrams[bigram],\n",
    "                      freq_unigrams[bigram[0]], p) -\n",
    "                log_f(freq_unigrams[bigram[1]] -\n",
    "                      freq_bigrams[bigram],\n",
    "                      len(words) - freq_unigrams[bigram[0]], p))\n",
    "    return lr\n",
    "\n",
    "\n",
    "def log_f(k, N, p):\n",
    "    return k * math.log(p) + (N - k) * math.log(1 - p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('det', 'var') \t 21108 \t 12090 \t 3840 \t 14948.40006538581\n",
      "('för', 'att') \t 9443 \t 28020 \t 2932 \t 9466.16287997624\n",
      "('ett', 'par') \t 5060 \t 794 \t 770 \t 7926.430303402298\n",
      "('det', 'är') \t 21108 \t 6290 \t 2015 \t 7711.859198590901\n",
      "('att', 'han') \t 28020 \t 21589 \t 2958 \t 4781.580514369096\n",
      "('han', 'hade') \t 21589 \t 13198 \t 2045 \t 4656.821489297785\n",
      "('en', 'gång') \t 13514 \t 1332 \t 706 \t 4177.6643694414415\n",
      "('hade', 'varit') \t 13198 \t 1721 \t 738 \t 3987.5749908637117\n",
      "('in', 'i') \t 2009 \t 16508 \t 790 \t 3745.32566457332\n",
      "('annat', 'än') \t 968 \t 2558 \t 409 \t 3570.1632253686766\n",
      "('därför', 'att') \t 864 \t 28020 \t 638 \t 3494.658526326093\n",
      "('sven', 'elversson') \t 343 \t 233 \t 216 \t 3469.873240404278\n",
      "('alla', 'fall') \t 2355 \t 295 \t 262 \t 2952.7612644543196\n",
      "('och', 'och') \t 36356 \t 36356 \t 3 \t 2937.1937788292416\n",
      "('en', 'sådan') \t 13514 \t 702 \t 451 \t 2917.190090611859\n"
     ]
    }
   ],
   "source": [
    "lr = likelihood_ratio(words, frequency, frequency_bigrams)\n",
    "\n",
    "for bigram in sorted(lr, key=lambda x: (-lr.get(x), x))[:15]:\n",
    "    print(bigram, \"\\t\", frequency[bigram[0]], \"\\t\", frequency[bigram[1]], \"\\t\",\n",
    "          frequency_bigrams[bigram], '\\t', lr[bigram])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_scores(words, freq_unigrams, freq_bigrams):\n",
    "    ts = {}\n",
    "    for bigram in freq_bigrams:\n",
    "        ts[bigram] = ((freq_bigrams[bigram] -\n",
    "                      freq_unigrams[bigram[0]] *\n",
    "                      freq_unigrams[bigram[1]] /\n",
    "                      len(words)) /\n",
    "                      math.sqrt(freq_bigrams[bigram]))\n",
    "    return ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('det', 'var') \t 21108 \t 12090 \t 3840 \t 57.50831814733698\n",
      "('för', 'att') \t 9443 \t 28020 \t 2932 \t 48.856597885438\n",
      "('att', 'han') \t 28020 \t 21589 \t 2958 \t 42.343471387453526\n",
      "('det', 'är') \t 21108 \t 6290 \t 2015 \t 41.685947391027824\n",
      "('han', 'hade') \t 21589 \t 13198 \t 2045 \t 38.39885253174174\n",
      "('att', 'hon') \t 28020 \t 13313 \t 1751 \t 32.191755461603314\n",
      "('att', 'det') \t 28020 \t 21108 \t 2038 \t 30.957451427524624\n",
      "('hon', 'hade') \t 13313 \t 13198 \t 1254 \t 30.039003436631656\n",
      "('så', 'att') \t 9149 \t 28020 \t 1264 \t 27.744803356194662\n",
      "('ett', 'par') \t 5060 \t 794 \t 770 \t 27.59209199153988\n",
      "('in', 'i') \t 2009 \t 16508 \t 790 \t 26.82923243495237\n",
      "('att', 'de') \t 28020 \t 11942 \t 1331 \t 26.55111022259168\n",
      "('hade', 'varit') \t 13198 \t 1721 \t 738 \t 26.26077526608472\n",
      "('en', 'gång') \t 13514 \t 1332 \t 706 \t 25.83706621960644\n",
      "('jag', 'har') \t 9511 \t 4666 \t 753 \t 25.689613879588258\n"
     ]
    }
   ],
   "source": [
    "ts = t_scores(words, frequency, frequency_bigrams)\n",
    "\n",
    "for bigram in sorted(ts, key=lambda x: (-ts.get(x), x))[:15]:\n",
    "    print(bigram, \"\\t\", frequency[bigram[0]], \"\\t\", frequency[bigram[1]], \"\\t\",\n",
    "          frequency_bigrams[bigram], '\\t', ts[bigram])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b97b11a820675205aae8f1d7f2a3f22bbd3a2c30189f44042310baf5b4cd1987"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
